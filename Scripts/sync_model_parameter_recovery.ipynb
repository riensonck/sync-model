{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sync-model-parameter-recovery.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpe9nP_5Z0Jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\"\"\"\n",
        "Author: Rien Sonck\n",
        "Date: 2020-06-28\n",
        "Description: This file aims to recover the parameters used to generate simulated data.  \n",
        "\"\"\"\n",
        "######################\n",
        "# Importing packages #\n",
        "######################\n",
        "from Sync_Model import model_sim\n",
        "from scipy.optimize import differential_evolution\n",
        "import pandas as pd\n",
        "from scipy import ndimage\n",
        "import numpy as np\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6ZCr5sOho1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def new_goal_func(x, *args):\n",
        "    ######################################\n",
        "    # Value Extraction and Initalization #\n",
        "    ######################################\n",
        "    \"\"\" \n",
        "    function that returns the error of the goal function based on previously generated \n",
        "    simulated data files by Sync_Model_Simulations.py\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x    : integer\n",
        "           this value lies in-between the specified bounds, and will be generated by the differential evolution algorithm. \n",
        "    args : tuple\n",
        "           any additional fixed model parameters and other variables needed to completely specify the objective function.\n",
        "    \"\"\" \n",
        "    gd = args[0]              # gamma distance model parameter\n",
        "    #thres = args[1]           # mfc frequency model parameter\n",
        "    target_file = args[1]     # target file\n",
        "    squared_err = []\n",
        "    mfc, thres = x[0], x[1]\n",
        "    ####################\n",
        "    # Reading in data #\n",
        "    ###################\n",
        "    # reading in the target data\n",
        "    pd_targetData = pd.read_csv(target_file)\n",
        "    ################################\n",
        "    # Generating model simulations #\n",
        "    ################################\n",
        "    pd_data = model_sim(Threshold = thres, drift = gd, theta_freq = mfc, save_behavioral = False, return_dataframe = True)\n",
        "    ###################################\n",
        "    # Calculating RT Shape difference #\n",
        "    ###################################\n",
        "    accuracy = pd_data['accuracy'].unique() \n",
        "    for acc in accuracy: \n",
        "        # extracting RTs\n",
        "        rt_sim = pd_data.loc[pd_data['accuracy'] == acc, 'rt'].values               # simulated RTs\n",
        "        rt_target = pd_targetData.loc[pd_targetData['accuracy'] == acc, 'rt'].values    # target RTs\n",
        "        # creating bins\n",
        "        bins = np.arange(0, 700.1, 25)\n",
        "        # getting histogram data\n",
        "        heights_sim, bins = np.histogram(rt_sim, bins=bins)\n",
        "        heights_target, bins = np.histogram(rt_target, bins=bins)\n",
        "        # calculating the difference\n",
        "        diff = heights_target - heights_sim\n",
        "        # calculating the squared error\n",
        "        err = sum(diff**2)\n",
        "        squared_err.append(err)\n",
        "\n",
        "    ###################################\n",
        "    # Calculating RT Peak difference  #\n",
        "    ###################################\n",
        "    # applying kernels to the error and correct RT distributions\n",
        "    # getting the right sigma for smoothing\n",
        "    cor_indices_target, err_indices_target = [], []\n",
        "    cor_indices_sim, err_indices_sim = [], []\n",
        "    bins = np.arange(0, 1000, 10)\n",
        "\n",
        "    sigma_cor = 5\n",
        "    sigma_err = 5\n",
        "    # this while loop will keep reducing the sigma until we can recover the second peak\n",
        "    while len(cor_indices_target) < 1 or len(err_indices_target) < 1:\n",
        "        RT_hist_list_target = RT_distribution_smoothing(pd_targetData, bins, sigma_err, sigma_cor)\n",
        "        # finding where in the bins the peaks and troughs happen and mark it \n",
        "        time_step_change_target = np.diff(np.sign(np.diff(RT_hist_list_target)))\n",
        "        # extracting the index of the peak and trough bins based on the marker\n",
        "        cor_indices_target = np.where((time_step_change_target[1]!= 0) & (time_step_change_target[1] != 1))[0]\n",
        "        err_indices_target = np.where((time_step_change_target[0] != 0) & (time_step_change_target[0] != 1))[0] \n",
        "        if len(cor_indices_target) < 1: \n",
        "            sigma_cor = sigma_cor - 1\n",
        "        if len(err_indices_target) < 1:\n",
        "            sigma_err = sigma_err - 1\n",
        "\n",
        "\n",
        "    sigma_cor = 5\n",
        "    sigma_err = 5\n",
        "    while len(cor_indices_sim) < 1 or len(err_indices_sim) < 1:\n",
        "        RT_hist_list_sim = RT_distribution_smoothing(pd_data, bins, sigma_err, sigma_cor)\n",
        "        # finding where in the bins the peaks and troughs happen and mark it \n",
        "        time_step_change_sim = np.diff(np.sign(np.diff(RT_hist_list_sim)))\n",
        "        # extracting the index of the peak and trough bins based on the marker\n",
        "        cor_indices_sim = np.where((time_step_change_sim[1]!= 0) & (time_step_change_sim[1] != 1))[0]\n",
        "        err_indices_sim = np.where((time_step_change_sim[0] != 0) & (time_step_change_sim[0] != 1))[0]\n",
        "        if len(cor_indices_sim) < 1: \n",
        "            sigma_cor = sigma_cor - 1\n",
        "        if len(err_indices_sim) < 1:\n",
        "            sigma_err = sigma_err - 1\n",
        "\n",
        "    # extracting the peaks and trough bin value\n",
        "    peaks_troughs_err_target = RT_hist_list_target[0][err_indices_target]\n",
        "    peaks_troughs_cor_target = RT_hist_list_target[1][cor_indices_target]\n",
        "    peaks_troughs_err_sim = RT_hist_list_sim[0][err_indices_sim]\n",
        "    peaks_troughs_cor_sim = RT_hist_list_sim[1][cor_indices_sim]\n",
        "    # calculate the relative peak height\n",
        "    #relative_height_cor_target = peaks_troughs_cor_target[1] / peaks_troughs_cor_target[0]\n",
        "    #relative_height_err_target = peaks_troughs_err_target[1] / peaks_troughs_err_target[0]\n",
        "    #relative_height_cor_sim = peaks_troughs_cor_sim[1] / peaks_troughs_cor_sim[0] \n",
        "    #relative_height_err_sim = peaks_troughs_err_sim[1] / peaks_troughs_err_sim[0]\n",
        "    relative_height_target = peaks_troughs_err_target[0] / peaks_troughs_cor_target[0]\n",
        "    relative_height_sim = peaks_troughs_err_sim[0] / peaks_troughs_cor_sim[0]\n",
        "    # error\n",
        "    #peak_error = np.abs(relative_height_cor_sim - relative_height_cor_target) + np.abs(relative_height_err_sim - relative_height_err_target)\n",
        "    peak_error = np.abs(relative_height_target - relative_height_sim)\n",
        "\n",
        "\n",
        "    # squared_err[1] is error, squared_err[0] is correct\n",
        "    # peak_error * 10**6 -\n",
        "    error = squared_err[1] + squared_err[0] + (peak_error * 10**4)\n",
        "    print(\"mfc: {0}, thres: {1}, gd: {2}, error: {3}\".format(mfc, thres, gd, error))\n",
        "    return error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY-otlWs_1Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RT_distribution_smoothing(pd_data, bins, sigma_err, sigma_cor):\n",
        "    \"\"\" \n",
        "    Function that applies the difference kernel and then a Gaussian smoothing kernel to the error \n",
        "    and correct RT distributions corresponding to each MFC frequency in the model.  \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pd_data    : pandas dataframe \n",
        "                 dataframe containing the data\n",
        "    bins       : numpy array\n",
        "                 array containing the bins for the histograms\n",
        "    sigma      : integer\n",
        "                 smoothing of the Gaussian kernel\n",
        "    \"\"\" \n",
        "    accuracy = [0, 1]\n",
        "    thetas_hist_list = []\n",
        "    theta_hist = np.zeros(shape=[2, 1, len(bins)-1])\n",
        "    ###################\n",
        "    # Reading in data #\n",
        "    ###################\n",
        "    # creating histograms\n",
        "    theta_hist[0, :] = np.histogram(pd_data.loc[pd_data['accuracy']== 0, 'rt'].values, bins=bins, normed = True)[0] # error histograms\n",
        "    theta_hist[1, :] = np.histogram(pd_data.loc[pd_data['accuracy']== 1, 'rt'].values, bins=bins, normed = True)[0] # correct histograms\n",
        "    ###################################\n",
        "    # Difference Kernel and Smoothing #\n",
        "    ###################################\n",
        "    # run difference kernel\n",
        "    theta_histDiff_err = -np.array([theta_hist[0][0][bin_n] - (theta_hist[0][0][bin_n-1] + theta_hist[0][0][bin_n+1]).mean() for bin_n in np.arange(1, len(bins)-2)]).T\n",
        "    theta_histDiff_cor = -np.array([theta_hist[1][0][bin_n] - (theta_hist[1][0][bin_n-1] + theta_hist[1][0][bin_n+1]).mean() for bin_n in np.arange(1, len(bins)-2)]).T\n",
        "    # smooth\n",
        "    theta_histDiffSmooth_err = ndimage.gaussian_filter1d(theta_histDiff_err, axis = 0, sigma = sigma_err)\n",
        "    theta_histDiffSmooth_cor = ndimage.gaussian_filter1d(theta_histDiff_cor, axis = 0, sigma = sigma_cor)\n",
        "    return (theta_histDiffSmooth_err, theta_histDiffSmooth_cor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQmcS1gv_-3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################\n",
        "# Value initialization #\n",
        "########################\n",
        "# path to the folder containing the target data\n",
        "path = ''\n",
        "mfc_freq_target = np.arange(8, 9, 1)\n",
        "thres_target = np.arange(4 , 5, 1)\n",
        "gds_target = np.arange(2, 3, 1)\n",
        "# model parameters\n",
        "bounds = [(3, 9), (1,5)]\n",
        "\n",
        "results = []\n",
        "for mfc in mfc_freq_target: \n",
        "  for gd in gds_target: \n",
        "    for thres in thres_target:\n",
        "      target_file = '{0}Behavioral_Data_simulation_sub0_thetaFreq{1}.00Hz_thresh{2}_drift{3}.0.csv'.format(path, mfc, thres, gd)\n",
        "      print(target_file)\n",
        "      print(\"=======\")\n",
        "      args =(gd, target_file) \t# arguments that the sync_func function needs to run (gamma distance, MFC frequency value, target dataframe)\n",
        "      #################\n",
        "      # Model Fitting #\n",
        "      #################\n",
        "      t = time.time()\n",
        "      result = differential_evolution(func=new_goal_func, bounds=bounds, args=args, workers = -1, popsize = 15, maxiter = 1000, updating = 'deferred', tol = 2, atol = 0)\n",
        "      d = {'mfc': mfc, 'gd': gd, 'thres': thres, \"mfc_result\": result.x}\n",
        "      data = pd.DataFrame(data=d)\n",
        "      results.append(data)\n",
        "      print(\"result: mfc = {0}, thres = {1}, mfc_result = {2}, thres_result = {3}\\n\".format(mfc, thres, result.x[0], result.x[1]))\n",
        "      print('\\ttime taken: %.2fmin' % ((time.time() - t) / 60.))\n",
        "pd_results = pd.concat(results, axis=0, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}